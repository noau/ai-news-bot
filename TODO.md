# AI News Bot - ä¼˜åŒ–ä»»åŠ¡æ¸…å•

> æœ¬æ–‡æ¡£åˆ—å‡ºäº†ä»£ç å®¡æŸ¥åå‘ç°çš„å¯ä¼˜åŒ–é¡¹ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº

## ğŸ“Š ä¼˜åŒ–æ¦‚è§ˆ

| ä¼˜å…ˆçº§ | ä»»åŠ¡æ•° | é¢„è®¡å½±å“ |
|--------|--------|----------|
| ğŸ”´ é«˜ä¼˜å…ˆçº§ | 3 | æ€§èƒ½æå‡ 3-5xï¼Œè´¨é‡æ˜¾è‘—æ”¹å–„ |
| ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ | 3 | å¯é æ€§å’Œç”¨æˆ·ä½“éªŒæå‡ |
| ğŸŸ¢ ä½ä¼˜å…ˆçº§ | 4 | é•¿æœŸæ”¹è¿›ï¼Œå¢å¼ºå¯è§‚æµ‹æ€§ |

---

## ğŸ”´ é«˜ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆç«‹å³å®æ–½ï¼‰

### 1. å®ç° RSS æºå¹¶å‘æŠ“å–

**å½±å“**ï¼šâ­â­â­â­â­ | **éš¾åº¦**ï¼šğŸ”§ğŸ”§ | **é¢„è®¡æå‡**ï¼šæ€§èƒ½æå‡ 3-5x

**å½“å‰é—®é¢˜**ï¼š
- æ–‡ä»¶ï¼š`src/news/fetcher.py:275-306`
- ç°çŠ¶ï¼šä¸²è¡ŒæŠ“å–æ‰€æœ‰ RSS æºï¼Œ20 ä¸ªæºéœ€è¦ 20-30 ç§’
- å½±å“ï¼šç”¨æˆ·ç­‰å¾…æ—¶é—´é•¿ï¼ŒGitHub Actions æ‰§è¡Œæ—¶é—´é•¿

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch_recent_news(self, language: str = "en", max_items_per_source: int = 5):
    all_news = {'international': [], 'domestic': []}

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {
            executor.submit(self.fetch_rss_feed, url, max_items_per_source): name
            for name, url in self.rss_feeds.items()
        }

        for future in as_completed(futures):
            source_name = futures[future]
            try:
                items = future.result(timeout=15)
                for item in items:
                    item['source'] = source_name
                    all_news['international'].append(item)
            except Exception as e:
                logger.error(f"Failed to fetch {source_name}: {e}")
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] æŠ“å–æ—¶é—´ä» 30ç§’ é™åˆ° 5-8ç§’
- [ ] æ‰€æœ‰ç°æœ‰ RSS æºéƒ½èƒ½æ­£å¸¸æŠ“å–
- [ ] å•ä¸ªæºå¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
- [ ] æ—¥å¿—æ­£ç¡®è®°å½•æ¯ä¸ªæºçš„çŠ¶æ€

**é¢„æœŸæ”¶ç›Š**ï¼š
- æ€»æ‰§è¡Œæ—¶é—´å‡å°‘ 60-70%
- ç”¨æˆ·ä½“éªŒæ˜¾è‘—æå‡
- GitHub Actions é…é¢ä½¿ç”¨å‡å°‘

---

### 2. æ·»åŠ æ–°é—»å»é‡æœºåˆ¶

**å½±å“**ï¼šâ­â­â­â­ | **éš¾åº¦**ï¼šğŸ”§ğŸ”§ | **é¢„è®¡æå‡**ï¼šè´¨é‡æå‡ï¼Œtoken èŠ‚çœ 10-20%

**å½“å‰é—®é¢˜**ï¼š
- æ–‡ä»¶ï¼š`src/news/generator.py`
- ç°çŠ¶ï¼šå¤šä¸ª RSS æºå¯èƒ½æŠ¥é“åŒä¸€äº‹ä»¶ï¼Œå¯¼è‡´é‡å¤å†…å®¹
- å½±å“ï¼šAI å¤„ç†é‡å¤æ–°é—»æµªè´¹ tokenï¼Œæœ€ç»ˆæ‘˜è¦å¯èƒ½åŒ…å«é‡å¤ä¿¡æ¯

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```python
def _deduplicate_news(self, news_items: List[Dict]) -> List[Dict]:
    """ä½¿ç”¨æ ‡é¢˜ç›¸ä¼¼åº¦å’Œé“¾æ¥å»é‡"""
    from difflib import SequenceMatcher

    unique_news = []
    seen_links = set()

    for item in news_items:
        # 1. é“¾æ¥å»é‡
        if item['link'] in seen_links:
            continue

        # 2. æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡
        is_duplicate = False
        for existing in unique_news:
            similarity = SequenceMatcher(
                None,
                item['title'].lower(),
                existing['title'].lower()
            ).ratio()

            if similarity > 0.85:  # 85% ç›¸ä¼¼åº¦é˜ˆå€¼
                is_duplicate = True
                # ä¿ç•™æè¿°æ›´è¯¦ç»†çš„ç‰ˆæœ¬
                if len(item.get('description', '')) > len(existing.get('description', '')):
                    unique_news.remove(existing)
                    unique_news.append(item)
                break

        if not is_duplicate:
            unique_news.append(item)
            seen_links.add(item['link'])

    logger.info(f"Deduplication: {len(news_items)} â†’ {len(unique_news)} items")
    return unique_news
```

**å®ç°ä½ç½®**ï¼š
åœ¨ `generator.py` çš„ `generate_news_digest_from_sources` æ–¹æ³•ä¸­ï¼Œ`fetch_recent_news` ä¹‹åã€æ ¼å¼åŒ–æ–°é—»ä¹‹å‰è°ƒç”¨

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] ç›¸åŒé“¾æ¥çš„æ–°é—»åªä¿ç•™ä¸€æ¡
- [ ] æ ‡é¢˜é«˜åº¦ç›¸ä¼¼ï¼ˆ>85%ï¼‰çš„æ–°é—»åªä¿ç•™æè¿°æ›´è¯¦ç»†çš„
- [ ] æ—¥å¿—è¾“å‡ºå»é‡å‰åçš„æ•°é‡å¯¹æ¯”
- [ ] ä¸è¯¯åˆ ä¸åŒçš„æ–°é—»

**é¢„æœŸæ”¶ç›Š**ï¼š
- æé«˜æ–°é—»æ‘˜è¦è´¨é‡
- å‡å°‘ 10-20% çš„ LLM token æ¶ˆè€—
- é¿å…ç”¨æˆ·çœ‹åˆ°é‡å¤å†…å®¹

---

### 3. å®ç° RSS è¯·æ±‚é‡è¯•æœºåˆ¶

**å½±å“**ï¼šâ­â­â­â­ | **éš¾åº¦**ï¼šğŸ”§ | **é¢„è®¡æå‡**ï¼šæˆåŠŸç‡æå‡ 15-30%

**å½“å‰é—®é¢˜**ï¼š
- æ–‡ä»¶ï¼š`src/news/fetcher.py:200`
- ç°çŠ¶ï¼šç½‘ç»œè¯·æ±‚å¤±è´¥åç›´æ¥è·³è¿‡è¯¥æºï¼Œæ— é‡è¯•
- å½±å“ï¼šä¸´æ—¶ç½‘ç»œé—®é¢˜å¯¼è‡´ä¸¢å¤±é‡è¦æ–°é—»æº

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼ˆäºŒé€‰ä¸€ï¼‰ï¼š

**æ–¹æ¡ˆ Aï¼šä½¿ç”¨ tenacity åº“**
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def fetch_rss_feed(self, feed_url: str, max_items: int = 10):
    # ç°æœ‰ä»£ç ...
```

**æ–¹æ¡ˆ Bï¼šæ‰‹åŠ¨å®ç°**
```python
def fetch_rss_feed_with_retry(self, feed_url: str, max_items: int = 10, retries: int = 3):
    for attempt in range(retries):
        try:
            return self.fetch_rss_feed(feed_url, max_items)
        except Exception as e:
            if attempt == retries - 1:
                logger.error(f"All {retries} attempts failed for {feed_url}")
                return []
            logger.warning(f"Attempt {attempt + 1} failed, retrying...")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿ï¼š2ç§’ã€4ç§’ã€8ç§’
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] ä¸´æ—¶ç½‘ç»œé”™è¯¯ä¼šè‡ªåŠ¨é‡è¯•
- [ ] ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
- [ ] æœ€å¤šé‡è¯• 3 æ¬¡
- [ ] æ—¥å¿—è®°å½•é‡è¯•æ¬¡æ•°å’Œæœ€ç»ˆç»“æœ

**ä¾èµ–**ï¼š
å»ºè®®å…ˆå®Œæˆ"å®ç° RSS æºå¹¶å‘æŠ“å–"ä»»åŠ¡

**é¢„æœŸæ”¶ç›Š**ï¼š
- æé«˜ RSS æŠ“å–æˆåŠŸç‡ 15-30%
- å‡å°‘å› ä¸´æ—¶ç½‘ç»œé—®é¢˜å¯¼è‡´çš„æ•°æ®ä¸¢å¤±

---

## ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆè¿‘æœŸå®æ–½ï¼‰

### 4. æ·»åŠ  RSS æŠ“å–ç¼“å­˜æœºåˆ¶

**å½±å“**ï¼šâ­â­â­ | **éš¾åº¦**ï¼šğŸ”§ğŸ”§ğŸ”§ | **é¢„è®¡æå‡**ï¼šå¼€å‘è°ƒè¯•é€Ÿåº¦ 10x

**å½“å‰é—®é¢˜**ï¼š
- æ¯æ¬¡è¿è¡Œéƒ½é‡æ–°æŠ“å–æ‰€æœ‰ RSS æº
- çŸ­æ—¶é—´å†…å¤šæ¬¡è¿è¡Œæµªè´¹ç½‘ç»œè¯·æ±‚
- è°ƒè¯•æ—¶åå¤æŠ“å–ç›¸åŒæ•°æ®

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```python
import hashlib
import json
from pathlib import Path
from datetime import datetime, timedelta

class NewsCache:
    def __init__(self, cache_dir: str = ".cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def get_rss_cache(self, feed_url: str, max_age_minutes: int = 30) -> Optional[List]:
        """è·å– RSS ç¼“å­˜ï¼ˆ30åˆ†é’Ÿå†…æœ‰æ•ˆï¼‰"""
        cache_key = hashlib.md5(feed_url.encode()).hexdigest()
        cache_file = self.cache_dir / f"rss_{cache_key}.json"

        if cache_file.exists():
            cache_data = json.loads(cache_file.read_text())
            cached_time = datetime.fromisoformat(cache_data['timestamp'])

            if datetime.now() - cached_time < timedelta(minutes=max_age_minutes):
                logger.info(f"Cache hit: {feed_url}")
                return cache_data['items']

        return None

    def set_rss_cache(self, feed_url: str, items: List):
        cache_key = hashlib.md5(feed_url.encode()).hexdigest()
        cache_file = self.cache_dir / f"rss_{cache_key}.json"

        cache_data = {
            'timestamp': datetime.now().isoformat(),
            'items': items
        }
        cache_file.write_text(json.dumps(cache_data, ensure_ascii=False))
```

**å®ç°ä½ç½®**ï¼š
- æ–°å»º `src/news/cache.py`
- åœ¨ `NewsFetcher.fetch_rss_feed` ä¸­é›†æˆ

**é…ç½®é¡¹**ï¼š
```env
ENABLE_RSS_CACHE=true  # å¯ç”¨ç¼“å­˜
RSS_CACHE_TTL=30       # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆåˆ†é’Ÿï¼‰
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] 30 åˆ†é’Ÿå†…é‡å¤è¿è¡Œä½¿ç”¨ç¼“å­˜
- [ ] è¶…è¿‡ 30 åˆ†é’Ÿè‡ªåŠ¨é‡æ–°æŠ“å–
- [ ] ç¼“å­˜æ–‡ä»¶ä¿å­˜åœ¨ `.cache/` ç›®å½•
- [ ] æ—¥å¿—æ˜¾ç¤ºç¼“å­˜å‘½ä¸­æƒ…å†µ
- [ ] å¯é€šè¿‡ç¯å¢ƒå˜é‡ç¦ç”¨ç¼“å­˜

**æ³¨æ„äº‹é¡¹**ï¼š
- å°† `.cache/` æ·»åŠ åˆ° `.gitignore`
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ç¦ç”¨ç¼“å­˜

**é¢„æœŸæ”¶ç›Š**ï¼š
- æœ¬åœ°å¼€å‘è°ƒè¯•æ—¶é€Ÿåº¦æå‡ 10x
- å‡å°‘å¯¹ RSS æºæœåŠ¡å™¨çš„å‹åŠ›

---

### 5. å¢å¼º Stage 1 å“åº”è§£æçš„å¥å£®æ€§

**å½±å“**ï¼šâ­â­â­ | **éš¾åº¦**ï¼šğŸ”§ğŸ”§ | **é¢„è®¡æå‡**ï¼šè§£æå¤±è´¥ç‡é™ä½ 80%

**å½“å‰é—®é¢˜**ï¼š
- æ–‡ä»¶ï¼š`src/news/generator.py:170-192`
- ç°çŠ¶ï¼šåªæœ‰ä¸€ç§ JSON è§£ææ–¹å¼ï¼ŒLLM è¾“å‡ºæ ¼å¼ä¸ç¬¦æ—¶å®¹æ˜“å¤±è´¥
- å½±å“ï¼šè§£æå¤±è´¥å fallback ç­–ç•¥è¿‡äºç®€å•

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

å®ç°ä¸‰å±‚é™çº§è§£æç­–ç•¥ï¼š

```python
def _parse_selection_response(self, response: str, news_items: dict) -> List[str]:
    """å¤šå±‚é™çº§çš„ ID è§£æ"""

    # å±‚çº§ 1: æ ‡å‡† JSON è§£æ
    json_match = re.search(r'\[[\s\S]*?\]', response)
    if json_match:
        try:
            selected_ids = json.loads(json_match.group(0))
            selected_ids = [id for id in selected_ids if id in news_items]
            if 15 <= len(selected_ids) <= 20:
                logger.info("Using JSON parsing")
                return selected_ids
        except json.JSONDecodeError:
            pass

    # å±‚çº§ 2: æ­£åˆ™è¡¨è¾¾å¼æå–
    id_pattern = r'(INT-\d+|DOM-\d+)'
    found_ids = re.findall(id_pattern, response)
    valid_ids = [id for id in found_ids if id in news_items]

    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique_ids = []
    for id in valid_ids:
        if id not in seen:
            seen.add(id)
            unique_ids.append(id)

    if 15 <= len(unique_ids) <= 20:
        logger.info("Using regex extraction")
        return unique_ids

    # å±‚çº§ 3: å¯å‘å¼æ™ºèƒ½é€‰æ‹©
    logger.warning("Falling back to heuristic selection")
    return self._heuristic_selection(news_items, target_count=18)

def _heuristic_selection(self, news_items: dict, target_count: int) -> List[str]:
    """åŸºäºå¯å‘å¼è§„åˆ™çš„æ™ºèƒ½é€‰æ‹©"""
    scored_items = []

    for id, item in news_items.items():
        score = 0.0

        # æè¿°é•¿åº¦ï¼ˆæ›´è¯¦ç»†çš„æ–°é—»ï¼‰
        score += len(item.get('description', '')) * 0.001

        # å…³é”®è¯åŒ¹é…
        high_value_keywords = ['breakthrough', 'release', 'launch', 'funding', 'acquisition', 'open-source']
        title_lower = item['title'].lower()
        score += sum(10 for kw in high_value_keywords if kw in title_lower)

        # æ¥æºæƒé‡
        premium_sources = ['OpenAI Blog', 'Google AI Blog', 'DeepMind Blog', 'Meta AI Blog']
        if item['source'] in premium_sources:
            score += 20

        scored_items.append((score, id))

    # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹© top N
    scored_items.sort(reverse=True)
    selected = [id for _, id in scored_items[:target_count]]

    # ç¡®ä¿å›½é™…/å›½å†…å¹³è¡¡
    int_count = sum(1 for id in selected if id.startswith('INT-'))
    dom_count = sum(1 for id in selected if id.startswith('DOM-'))

    logger.info(f"Heuristic selection: {int_count} international, {dom_count} domestic")
    return selected
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] JSON æ ¼å¼æ­£ç¡®æ—¶ä¼˜å…ˆä½¿ç”¨
- [ ] JSON å¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ°æ­£åˆ™æå–
- [ ] æ­£åˆ™å¤±è´¥æ—¶ä½¿ç”¨å¯å‘å¼é€‰æ‹©
- [ ] æ—¥å¿—è®°å½•ä½¿ç”¨çš„è§£ææ–¹æ³•
- [ ] ä»»ä½•æƒ…å†µéƒ½èƒ½è¿”å› 15-20 æ¡æ–°é—»

**é¢„æœŸæ”¶ç›Š**ï¼š
- è§£æå¤±è´¥ç‡é™ä½ 80%
- é™çº§æ–¹æ¡ˆè´¨é‡æ¥è¿‘ LLM é€‰æ‹©
- æ›´ç¨³å®šçš„ç”Ÿäº§è¿è¡Œ

---

### 6. æ·»åŠ å¯åŠ¨é…ç½®éªŒè¯

**å½±å“**ï¼šâ­â­â­ | **éš¾åº¦**ï¼šğŸ”§ | **é¢„è®¡æå‡**ï¼šç”¨æˆ·ä½“éªŒæ˜¾è‘—æå‡

**å½“å‰é—®é¢˜**ï¼š
- æ–‡ä»¶ï¼š`src/config.py`
- ç°çŠ¶ï¼šç¼ºå°‘é…ç½®å¿…è¦æ€§æ£€æŸ¥ï¼Œè¿è¡Œåˆ°ä¸€åŠæ‰å‘ç°é…ç½®ç¼ºå¤±
- å½±å“ï¼šæµªè´¹ RSS æŠ“å–æ—¶é—´å’Œ LLM token

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
class Config:
    def __init__(self, config_path: Optional[str] = None):
        load_dotenv()
        self.config_path = self._find_config_file(config_path)
        self.config_data = self._load_yaml_config()

        # æ–°å¢ï¼šéªŒè¯é…ç½®
        self._validate_config()

        logger.info(f"Configuration loaded from {self.config_path}")

    def _validate_config(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        errors = []

        # 1. æ£€æŸ¥ LLM é…ç½®
        provider = self.llm_provider
        api_key = self.llm_api_key

        if not api_key:
            errors.append(
                f"Missing API key for provider '{provider}'. "
                f"Set {provider.upper()}_API_KEY in environment."
            )

        # 2. æ£€æŸ¥é€šçŸ¥é…ç½®
        methods = self.notification_methods
        if not methods:
            errors.append(
                "No notification methods configured. "
                "Set NOTIFICATION_METHODS in environment (e.g., 'email,slack')."
            )

        if "email" in methods:
            if not all([os.getenv("GMAIL_ADDRESS"),
                       os.getenv("GMAIL_APP_PASSWORD"),
                       os.getenv("EMAIL_TO")]):
                errors.append(
                    "Email notification enabled but missing credentials. "
                    "Required: GMAIL_ADDRESS, GMAIL_APP_PASSWORD, EMAIL_TO"
                )

        if "slack" in methods and not os.getenv("SLACK_WEBHOOK_URL"):
            errors.append("Slack enabled but SLACK_WEBHOOK_URL not set")

        if "telegram" in methods:
            if not all([os.getenv("TELEGRAM_BOT_TOKEN"), os.getenv("TELEGRAM_CHAT_ID")]):
                errors.append("Telegram enabled but missing TELEGRAM_BOT_TOKEN or TELEGRAM_CHAT_ID")

        if "discord" in methods and not os.getenv("DISCORD_WEBHOOK_URL"):
            errors.append("Discord enabled but DISCORD_WEBHOOK_URL not set")

        if "webhook" in methods and not os.getenv("WEBHOOK_URL"):
            errors.append("Webhook enabled but WEBHOOK_URL not set")

        # 3. æ£€æŸ¥è¯­è¨€é…ç½®
        languages = self.ai_response_languages
        if not languages:
            errors.append("No valid languages configured (AI_RESPONSE_LANGUAGE)")

        # 4. æŠ¥å‘Šé”™è¯¯
        if errors:
            error_msg = "\nâŒ Configuration validation failed:\n\n" + "\n".join(f"  â€¢ {e}" for e in errors)
            error_msg += "\n\nPlease fix the configuration and try again.\n"
            logger.error(error_msg)
            raise ValueError(error_msg)

        logger.info("âœ“ Configuration validation passed")
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] æ‰€æœ‰å¿…éœ€é…ç½®ç¼ºå¤±æ—¶æ— æ³•å¯åŠ¨
- [ ] é”™è¯¯æ¶ˆæ¯æ¸…æ™°ï¼Œåˆ—å‡ºæ‰€æœ‰ç¼ºå¤±é¡¹
- [ ] é…ç½®å®Œæ•´æ—¶æ­£å¸¸è¿è¡Œ
- [ ] æ—¥å¿—è¾“å‡ºéªŒè¯é€šè¿‡æ ‡å¿—

**é¢„æœŸæ”¶ç›Š**ï¼š
- å¿«é€Ÿå‘ç°é…ç½®é—®é¢˜ï¼ˆå¯åŠ¨æ—¶ vs è¿è¡Œ 30 ç§’åï¼‰
- èŠ‚çœ RSS æŠ“å–æ—¶é—´å’Œ LLM token
- æ–°ç”¨æˆ·é…ç½®ä½“éªŒæ›´å¥½

---

## ğŸŸ¢ ä½ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆé•¿æœŸæ”¹è¿›ï¼‰

### 7. å®ç°æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†

**å½±å“**ï¼šâ­â­ | **éš¾åº¦**ï¼šğŸ”§ğŸ”§ | **ç±»å‹**ï¼šå¯è§‚æµ‹æ€§

**ç›®æ ‡**ï¼šæ”¶é›†æ€§èƒ½æŒ‡æ ‡ï¼Œå‘ç°ç“¶é¢ˆï¼Œä¼˜åŒ–æˆæœ¬

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
class MetricsCollector:
    def __init__(self):
        self.metrics = {
            'rss_sources': [],
            'llm_stage1_time': 0,
            'llm_stage2_time': 0,
            'llm_tokens': {'input': 0, 'output': 0},
            'news_counts': {
                'fetched': 0,
                'deduplicated': 0,
                'selected': 0,
                'final_chars': 0
            },
            'notifications': []
        }

    def record_rss_fetch(self, source: str, duration: float, count: int, success: bool):
        self.metrics['rss_sources'].append({
            'source': source,
            'duration': duration,
            'count': count,
            'success': success
        })

    def record_llm_stage(self, stage: int, duration: float, tokens: dict):
        self.metrics[f'llm_stage{stage}_time'] = duration
        self.metrics['llm_tokens']['input'] += tokens.get('input', 0)
        self.metrics['llm_tokens']['output'] += tokens.get('output', 0)

    def export_metrics(self, output_file: str = ".metrics/run.json"):
        """å¯¼å‡ºæŒ‡æ ‡åˆ° JSON æ–‡ä»¶"""
        output_path = Path(output_file)
        output_path.parent.mkdir(exist_ok=True)

        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_time': sum([
                self.metrics['llm_stage1_time'],
                self.metrics['llm_stage2_time']
            ]),
            'rss_summary': {
                'total_sources': len(self.metrics['rss_sources']),
                'successful': sum(1 for s in self.metrics['rss_sources'] if s['success']),
                'avg_duration': sum(s['duration'] for s in self.metrics['rss_sources']) / len(self.metrics['rss_sources'])
            },
            'llm_summary': {
                'total_tokens': self.metrics['llm_tokens']['input'] + self.metrics['llm_tokens']['output'],
                'estimated_cost': self._estimate_cost()
            },
            'detailed_metrics': self.metrics
        }

        output_path.write_text(json.dumps(summary, indent=2, ensure_ascii=False))
        logger.info(f"Metrics exported to {output_file}")
        return summary

    def _estimate_cost(self) -> float:
        """ä¼°ç®— Claude API æˆæœ¬"""
        input_tokens = self.metrics['llm_tokens']['input']
        output_tokens = self.metrics['llm_tokens']['output']

        # Claude Sonnet 4.5 pricing: $3/MTok input, $15/MTok output
        cost = (input_tokens / 1_000_000 * 3) + (output_tokens / 1_000_000 * 15)
        return round(cost, 4)
```

**é›†æˆä½ç½®**ï¼š
- æ–°å»º `src/metrics.py`
- åœ¨ `main.py` ä¸­åˆå§‹åŒ–å¹¶ä½¿ç”¨
- å„æ¨¡å—æ·»åŠ  `metrics.record_xxx()` è°ƒç”¨

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] æ¯æ¬¡è¿è¡Œç”Ÿæˆ `.metrics/run.json`
- [ ] åŒ…å«æ‰€æœ‰å…³é”®æ€§èƒ½æŒ‡æ ‡
- [ ] å¯ä»¥åˆ†æå¤šæ¬¡è¿è¡Œçš„è¶‹åŠ¿
- [ ] ä¸å½±å“ä¸»æµç¨‹æ€§èƒ½ï¼ˆ<1% å¼€é”€ï¼‰

**é¢„æœŸæ”¶ç›Š**ï¼š
- æ•°æ®é©±åŠ¨çš„ä¼˜åŒ–å†³ç­–
- å‘ç°æ€§èƒ½ç“¶é¢ˆ
- ç›‘æ§ LLM token æˆæœ¬

---

### 8. æ·»åŠ æ–°é—»è´¨é‡é¢„è¯„åˆ†æœºåˆ¶

**å½±å“**ï¼šâ­â­ | **éš¾åº¦**ï¼šğŸ”§ğŸ”§ğŸ”§ | **ç±»å‹**ï¼šè´¨é‡ä¼˜åŒ–

**ç›®æ ‡**ï¼šåœ¨ Stage 1 ä¹‹å‰é¢„ç­›é€‰ï¼Œå‡å°‘ä½è´¨é‡æ–°é—»çš„ LLM åˆ†æ

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
def _score_news_item(self, item: dict) -> float:
    """
    è¯„ä¼°æ–°é—»è´¨é‡ï¼ˆ0-1 åˆ†ï¼‰

    è¯„åˆ†ç»´åº¦ï¼š
    - å…³é”®è¯æƒé‡ (0-0.3)
    - æ¥æºæƒé‡ (0-0.3)
    - å†…å®¹è´¨é‡ (0-0.2)
    - æ—¶æ•ˆæ€§ (0-0.2)
    """
    score = 0.0

    # 1. å…³é”®è¯æƒé‡
    title_lower = item['title'].lower()
    desc_lower = item.get('description', '').lower()

    high_value_keywords = {
        'breakthrough', 'release', 'launch', 'funding', 'acquisition',
        'open-source', 'announce', 'introduce', 'unveil'
    }
    tech_keywords = {
        'gpt', 'claude', 'llm', 'transformer', 'agent', 'multimodal',
        'reasoning', 'benchmark', 'sota', 'api'
    }

    kw_score = 0
    kw_score += sum(0.05 for kw in high_value_keywords if kw in title_lower)
    kw_score += sum(0.03 for kw in tech_keywords if kw in title_lower)
    score += min(kw_score, 0.3)

    # 2. æ¥æºæƒé‡
    source_tiers = {
        'tier1': ['OpenAI Blog', 'Google AI Blog', 'DeepMind Blog', 'Meta AI Blog', 'Microsoft AI Blog'],
        'tier2': ['TechCrunch AI', 'VentureBeat AI', 'MIT Technology Review', 'arXiv'],
        'tier3': ['The Verge AI', 'Ars Technica AI', 'Wired AI']
    }

    if item['source'] in source_tiers['tier1']:
        score += 0.3
    elif item['source'] in source_tiers['tier2']:
        score += 0.2
    elif item['source'] in source_tiers['tier3']:
        score += 0.1

    # 3. å†…å®¹è´¨é‡
    desc_len = len(item.get('description', ''))
    if desc_len > 500:
        score += 0.1
    elif desc_len > 300:
        score += 0.05

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æ•°å­—/æŒ‡æ ‡
    if re.search(r'\d+%|\d+x|\$\d+[MB]|\d+\s*(billion|million)', desc_lower):
        score += 0.1

    # 4. æ—¶æ•ˆæ€§ï¼ˆåŸºäºå‘å¸ƒæ—¶é—´ï¼‰
    # TODO: å®ç°æ—¶é—´è§£æå’Œè¯„åˆ†

    return min(score, 1.0)

def _filter_by_quality(self, news_items: List[Dict], min_score: float = 0.3) -> List[Dict]:
    """è¿‡æ»¤ä½è´¨é‡æ–°é—»"""
    scored = []
    for item in news_items:
        score = self._score_news_item(item)
        if score >= min_score:
            scored.append((score, item))

    # æŒ‰åˆ†æ•°æ’åº
    scored.sort(reverse=True, key=lambda x: x[0])

    # è®°å½•ç»Ÿè®¡
    logger.info(f"Quality filtering: {len(news_items)} â†’ {len(scored)} items (threshold: {min_score})")
    logger.debug(f"Score distribution: {self._score_distribution([s for s, _ in scored])}")

    return [item for _, item in scored]

def _score_distribution(self, scores: List[float]) -> dict:
    """è®¡ç®—åˆ†æ•°åˆ†å¸ƒ"""
    bins = {'0-0.3': 0, '0.3-0.5': 0, '0.5-0.7': 0, '0.7-1.0': 0}
    for score in scores:
        if score < 0.3:
            bins['0-0.3'] += 1
        elif score < 0.5:
            bins['0.3-0.5'] += 1
        elif score < 0.7:
            bins['0.5-0.7'] += 1
        else:
            bins['0.7-1.0'] += 1
    return bins
```

**ä½¿ç”¨ä½ç½®**ï¼š
åœ¨ `generate_news_digest_from_sources` ä¸­ï¼Œ`_format_news_with_ids` ä¹‹å‰è°ƒç”¨

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] é«˜è´¨é‡æ–°é—»ï¼ˆå®˜æ–¹åšå®¢ã€é‡å¤§å‘å¸ƒï¼‰å¾—åˆ† > 0.7
- [ ] ä½è´¨é‡æ–°é—»ï¼ˆä¼ é—»ã€è§‚ç‚¹ï¼‰å¾—åˆ† < 0.3
- [ ] è¿‡æ»¤åä¿ç•™è‡³å°‘ 40-50 æ¡ä¾› Stage 1 é€‰æ‹©
- [ ] æ—¥å¿—è¾“å‡ºåˆ†æ•°åˆ†å¸ƒç»Ÿè®¡

**é¢„æœŸæ”¶ç›Š**ï¼š
- å‡å°‘ Stage 1 çš„ token æ¶ˆè€— 20-30%
- æé«˜æœ€ç»ˆæ‘˜è¦çš„å¹³å‡è´¨é‡

---

### 9. å®ç°å¢é‡æ›´æ–°æ¨¡å¼

**å½±å“**ï¼šâ­â­ | **éš¾åº¦**ï¼šğŸ”§ğŸ”§ğŸ”§ | **ç±»å‹**ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆé«˜é¢‘åœºæ™¯ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šæœ¬åœ°å¼€å‘ã€è‡ªå»ºå®šæ—¶ä»»åŠ¡ï¼ˆæ¯å°æ—¶/æ¯4å°æ—¶ï¼‰

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
class IncrementalUpdater:
    def __init__(self, state_file: str = ".last_run"):
        self.state_file = Path(state_file)

    def get_last_run_time(self) -> Optional[datetime]:
        """è¯»å–ä¸Šæ¬¡è¿è¡Œæ—¶é—´"""
        if not self.state_file.exists():
            return None

        try:
            data = json.loads(self.state_file.read_text())
            return datetime.fromisoformat(data['last_run_time'])
        except Exception as e:
            logger.warning(f"Failed to read last run time: {e}")
            return None

    def save_run_time(self, stats: dict):
        """ä¿å­˜è¿è¡Œæ—¶é—´å’Œç»Ÿè®¡"""
        data = {
            'last_run_time': datetime.now().isoformat(),
            'items_fetched': stats.get('fetched', 0),
            'items_selected': stats.get('selected', 0)
        }
        self.state_file.write_text(json.dumps(data, indent=2))

def fetch_news_since(
    self,
    since_time: Optional[datetime] = None,
    min_items: int = 10
) -> Dict[str, List[Dict]]:
    """
    å¢é‡æŠ“å–ï¼šåªè·å–æŒ‡å®šæ—¶é—´åçš„æ–°é—»

    Args:
        since_time: èµ·å§‹æ—¶é—´ï¼ŒNone åˆ™å…¨é‡æŠ“å–
        min_items: æœ€å°æ–°é—»æ•°ï¼Œä¸è¶³åˆ™å›é€€å…¨é‡
    """
    if since_time is None:
        logger.info("Incremental mode: disabled, doing full fetch")
        return self.fetch_recent_news()

    logger.info(f"Incremental mode: fetching news since {since_time.isoformat()}")

    all_news = self.fetch_recent_news()

    # è¿‡æ»¤æ—§æ–°é—»
    filtered = {'international': [], 'domestic': []}

    for category in ['international', 'domestic']:
        for item in all_news[category]:
            pub_time = self._parse_pub_time(item.get('published'))
            if pub_time and pub_time > since_time:
                filtered[category].append(item)

    total_items = len(filtered['international']) + len(filtered['domestic'])

    # æ–°é—»ä¸è¶³ï¼Œå›é€€å…¨é‡
    if total_items < min_items:
        logger.warning(
            f"Incremental fetch returned only {total_items} items (< {min_items}), "
            f"falling back to full fetch"
        )
        return all_news

    logger.info(f"Incremental fetch: {total_items} new items since last run")
    return filtered

def _parse_pub_time(self, pub_string: str) -> Optional[datetime]:
    """è§£æå„ç§ RSS å‘å¸ƒæ—¶é—´æ ¼å¼"""
    if not pub_string:
        return None

    # å°è¯•å¤šç§æ ¼å¼
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",  # RSS 2.0
        "%Y-%m-%dT%H:%M:%S%z",        # ISO 8601
        "%Y-%m-%d %H:%M:%S",          # ç®€å•æ ¼å¼
    ]

    for fmt in formats:
        try:
            return datetime.strptime(pub_string, fmt)
        except ValueError:
            continue

    logger.debug(f"Failed to parse time: {pub_string}")
    return None
```

**é…ç½®é¡¹**ï¼š
```env
ENABLE_INCREMENTAL=true  # å¯ç”¨å¢é‡æ¨¡å¼
INCREMENTAL_MIN_ITEMS=10 # æœ€å°‘æ–°é—»æ•°é˜ˆå€¼
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] é¦–æ¬¡è¿è¡Œæˆ–æ–‡ä»¶ç¼ºå¤±æ—¶å…¨é‡æŠ“å–
- [ ] åç»­è¿è¡Œåªå¤„ç†æ–°æ–°é—»
- [ ] æ–°é—»ä¸è¶³æ—¶è‡ªåŠ¨å›é€€å…¨é‡
- [ ] æ—¥å¿—æ¸…æ™°æ˜¾ç¤ºå¢é‡ vs å…¨é‡æ¨¡å¼

**é¢„æœŸæ”¶ç›Š**ï¼š
- é«˜é¢‘è¿è¡Œåœºæ™¯èŠ‚çœ 70-80% èµ„æº
- é€‚åˆæ„å»ºå®æ—¶æ–°é—»ç³»ç»Ÿ

---

### 10. æ”¹è¿›é”™è¯¯æ¢å¤å’Œéƒ¨åˆ†å¤±è´¥å®¹å¿

**å½±å“**ï¼šâ­â­ | **éš¾åº¦**ï¼šğŸ”§ğŸ”§ | **ç±»å‹**ï¼šå¯é æ€§

**ç›®æ ‡**ï¼šæé«˜ç³»ç»Ÿé²æ£’æ€§ï¼Œé¿å…å•ç‚¹æ•…éšœå¯¼è‡´å®Œå…¨å¤±è´¥

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

**1. LLM API é‡è¯•**
```python
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=30),
    retry=retry_if_exception_type((APIConnectionError, APITimeoutError))
)
def generate(self, messages: List[Dict], **kwargs) -> str:
    """å¸¦é‡è¯•çš„ LLM API è°ƒç”¨"""
    # ç°æœ‰å®ç°
```

**2. ç»†åŒ–é€€å‡ºç **
```python
# main.py
EXIT_CODE_SUCCESS = 0           # å…¨éƒ¨æˆåŠŸ
EXIT_CODE_PARTIAL_SUCCESS = 0   # éƒ¨åˆ†æˆåŠŸï¼ˆå¯æ¥å—ï¼‰
EXIT_CODE_PARTIAL_FAILURE = 1   # éƒ¨åˆ†å¤±è´¥ï¼ˆéœ€å…³æ³¨ï¼‰
EXIT_CODE_TOTAL_FAILURE = 2     # å®Œå…¨å¤±è´¥ï¼ˆéœ€å‘Šè­¦ï¼‰

def main():
    # ...

    # è®¡ç®—æˆåŠŸç‡
    total_attempts = len(languages) * len(notification_methods)
    success_count = len(overall_results["sent"])
    success_rate = success_count / total_attempts if total_attempts > 0 else 0

    if success_rate == 1.0:
        logger.info("âœ“ All notifications sent successfully")
        return EXIT_CODE_SUCCESS
    elif success_rate >= 0.7:
        logger.info(f"âš  Partial success ({success_rate:.0%})")
        return EXIT_CODE_PARTIAL_SUCCESS
    elif success_rate > 0:
        logger.warning(f"âš  Partial failure ({success_rate:.0%})")
        return EXIT_CODE_PARTIAL_FAILURE
    else:
        logger.error("âœ— Total failure - no notifications sent")
        return EXIT_CODE_TOTAL_FAILURE
```

**3. é™çº§ç­–ç•¥**
```python
def generate_news_digest_from_sources(self, ...):
    try:
        # å°è¯•ä¸¤é˜¶æ®µç”Ÿæˆ
        return self._two_stage_generation(...)
    except Exception as e:
        logger.error(f"Two-stage generation failed: {e}")

        # é™çº§åˆ°ç®€åŒ–ç‰ˆ
        logger.info("Falling back to simplified digest")
        return self._generate_simplified_digest(news_data)

def _generate_simplified_digest(self, news_data: dict) -> str:
    """é™çº§æ–¹æ¡ˆï¼šä»…æ ‡é¢˜å’Œé“¾æ¥"""
    digest = "# AI News Digest (Simplified)\n\n"

    for category in ['international', 'domestic']:
        if news_data[category]:
            digest += f"## {category.title()} News\n\n"
            for i, item in enumerate(news_data[category][:20], 1):
                digest += f"{i}. [{item['title']}]({item['link']})\n"
                digest += f"   Source: {item['source']}\n\n"

    return digest
```

**4. æœ¬åœ°æ–‡ä»¶å¤‡ä»½**
```python
def save_digest_to_file(digest: str, language: str):
    """æ‰€æœ‰é€šçŸ¥å¤±è´¥æ—¶ä¿å­˜åˆ°æ–‡ä»¶"""
    output_dir = Path(".output")
    output_dir.mkdir(exist_ok=True)

    filename = f"digest_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{language}.md"
    output_file = output_dir / filename

    output_file.write_text(digest, encoding='utf-8')
    logger.info(f"Digest saved to {output_file}")
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] LLM API ä¸´æ—¶é”™è¯¯è‡ªåŠ¨é‡è¯•
- [ ] Stage 1 å¤±è´¥èƒ½é™çº§
- [ ] æ‰€æœ‰é€šçŸ¥å¤±è´¥æ—¶ä¿å­˜åˆ°æ–‡ä»¶
- [ ] é€€å‡ºç å‡†ç¡®åæ˜ æ‰§è¡ŒçŠ¶æ€
- [ ] GitHub Actions æ—¥å¿—æ¸…æ™°

**é¢„æœŸæ”¶ç›Š**ï¼š
- å‡å°‘å› å•ç‚¹æ•…éšœå¯¼è‡´çš„å®Œå…¨å¤±è´¥
- æ›´å¥½çš„æ•…éšœæ’æŸ¥
- é™ä½äººå·¥ä»‹å…¥éœ€æ±‚

---

## ğŸ“ å®æ–½å»ºè®®

### ç«‹å³å®æ–½ï¼ˆæœ¬å‘¨ï¼‰
1. âœ… å®ç° RSS æºå¹¶å‘æŠ“å–
2. âœ… æ·»åŠ æ–°é—»å»é‡æœºåˆ¶
3. âœ… å®ç° RSS è¯·æ±‚é‡è¯•æœºåˆ¶

### è¿‘æœŸå®æ–½ï¼ˆæœ¬æœˆï¼‰
4. â­• æ·»åŠ  RSS æŠ“å–ç¼“å­˜æœºåˆ¶
5. â­• å¢å¼º Stage 1 å“åº”è§£æ
6. â­• æ·»åŠ å¯åŠ¨é…ç½®éªŒè¯

### é•¿æœŸè§„åˆ’ï¼ˆæŒ‰éœ€ï¼‰
7. â¸ æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†
8. â¸ æ–°é—»è´¨é‡é¢„è¯„åˆ†
9. â¸ å¢é‡æ›´æ–°æ¨¡å¼
10. â¸ é”™è¯¯æ¢å¤ä¼˜åŒ–

---

## ğŸ¯ é¢„æœŸæ•´ä½“æ”¶ç›Š

å®Œæˆå‰ 6 é¡¹ä¼˜åŒ–åï¼š
- âš¡ æ€§èƒ½ï¼šæ€»æ‰§è¡Œæ—¶é—´å‡å°‘ **60-70%**
- ğŸ’° æˆæœ¬ï¼šLLM token æ¶ˆè€—å‡å°‘ **15-25%**
- ğŸ“ˆ è´¨é‡ï¼šæ–°é—»æ‘˜è¦è´¨é‡æå‡ **20-30%**
- ğŸ›¡ï¸ å¯é æ€§ï¼šé”™è¯¯ç‡é™ä½ **80%**
- ğŸ‘¥ ä½“éªŒï¼šç”¨æˆ·é…ç½®é”™è¯¯æå‰å‘ç°ï¼Œä½“éªŒæå‡

---

**æœ€åæ›´æ–°**ï¼š2026-02-11
**è´Ÿè´£äºº**ï¼šå¾…åˆ†é…
**çŠ¶æ€**ï¼šå¾…å¼€å§‹
